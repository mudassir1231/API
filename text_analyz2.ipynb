{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNLoYOYuamh8g/ZLsYDyUa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhT8JW4Xa3Qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8f5806-0996-400a-d558-fda0743456f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl as xl\n",
        "from openpyxl.chart import BarChart, Reference\n",
        "import xlrd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "ioEsyXjS6fU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from operator import indexOf\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "metadata": {
        "id": "oOcRMu9W8ydu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PfzLuzf7bn0",
        "outputId": "6c1cdea2-6452-4e9a-8e1c-4c4555c02ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the url`s\n",
        "loc = (\"/content/drive/MyDrive/code/20211030 Test Assignment/Input.xlsx\")\n",
        "# to open workbook\n",
        "wb = xlrd.open_workbook(loc)\n",
        "sheet = wb.sheet_by_index(0)\n",
        "# For row 0 and column 0\n",
        "\n",
        "def par(url,i):\n",
        "  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',}\n",
        "  r = requests.get(url, headers=headers)\n",
        "  htmlContent = r.content\n",
        "  soup = BeautifulSoup(htmlContent, 'html.parser')\n",
        "  paras = soup.find_all('p')\n",
        "  reviews = [result.text for result in paras]\n",
        "  # for i in paras:\n",
        "  #   # print(len(i.text))\n",
        "  #   # print(para.text.encode(\"utf-8\"))\n",
        "  #   print(i.text)\n",
        "    "
      ],
      "metadata": {
        "id": "HDQCt9-P6v4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the positive words and negative words\n",
        "pathn='/content/drive/MyDrive/code/20211030 Test Assignment/MasterDictionary/negative-words.txt'\n",
        "pathp='/content/drive/MyDrive/code/20211030 Test Assignment/MasterDictionary/positive-words.txt'\n",
        "pos=['a+']\n",
        "neg=['2-faced']\n",
        "poss=0\n",
        "negg=0\n",
        "textp= pd.read_csv(pathp, sep='|', encoding='latin-1')\n",
        "for i in textp['a+']:\n",
        "  pos.append(i.strip())\n",
        "\n",
        "textn= pd.read_csv(pathn, sep='|', encoding='latin-1')\n",
        "for n in textn['2-faced']:\n",
        "  neg.append(n.strip())\n",
        "\n"
      ],
      "metadata": {
        "id": "cDVdRpBk8pp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get string of stop words\n",
        "# list all files in the directory\n",
        "\n",
        "path='/content/drive/MyDrive/code/20211030 Test Assignment/StopWords/'\n",
        "files = os.listdir(path)\n",
        "# print(files)\n",
        "stoptext1=[]\n",
        "\n",
        "for i in files:\n",
        "    f = open(path+i, 'r')\n",
        "    text= pd.read_csv(path+i, sep='|', encoding='latin-1')\n",
        "    x=[line.strip() for line in text]\n",
        "    stoptext1.append(x)\n",
        "    f.close()\n",
        "\n",
        "print(stoptext1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ478OFe77VZ",
        "outputId": "c71b1d74-f091-4602-8fc6-fcb19c8c5948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['SMITH', 'Surnames from 1990 census > .002%.  www.census.gov.genealogy/names/dist.all.last'], ['UNITED', 'Geographic'], ['a'], ['HUNDRED', 'Denominations'], ['AFGHANI', 'Afghanistan'], ['ERNST'], ['ABOUT']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exlm=['?','!','.','/','-','`','(',')','...','..','....','@','_']"
      ],
      "metadata": {
        "id": "srDbQ9O58MiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove stop words\n",
        "def rem_stop_word(tokens):\n",
        "  for i in tokens:\n",
        "    for j in stoptext1:\n",
        "      for k in j:\n",
        "        if(i==k):\n",
        "          tokens.remove(i)\n",
        "    for ex in exlm:\n",
        "        if(i==ex):\n",
        "          tokens.remove(i)"
      ],
      "metadata": {
        "id": "MZTu-ZAN8Qa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to get sillibals\n",
        "def syllable_count(word):\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "    vowels = \"aeiouy\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "# print(syllable_count('chocolate'))"
      ],
      "metadata": {
        "id": "a9-8hUfu83cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'URL':['test'],\n",
        "        'Polarity Score':[0],\n",
        "        'Subjectivity Score':[0],\n",
        "        'Average Sentence Length':[0],\n",
        "        '% of Complex words':[0],\n",
        "        'Fog Index':[0],\n",
        "        'Average Words/Sentence':[0]}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "k9g8_bBiJUab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text analysis\n",
        "\n",
        "def score(stri,url):\n",
        "  poss=0\n",
        "  negg=0\n",
        "  cmplex=0\n",
        "  se=[]\n",
        "  seew=[]\n",
        "  se1=[]\n",
        "  results=[]\n",
        "  sent= nltk.sent_tokenize(stri)\n",
        "  for sen in sent:\n",
        "    se.append(nltk.sent_tokenize(sen))\n",
        "    seew=word_tokenize(stri)\n",
        "  rem_stop_word(seew)\n",
        "\n",
        "  tokens = nltk.word_tokenize(stri)\n",
        "  rem_stop_word(tokens)\n",
        "\n",
        "  for i in tokens:\n",
        "    for j in pos:\n",
        "      if(i==j):\n",
        "        poss=+1\n",
        "        # print(j)\n",
        "\n",
        "  for i in tokens:\n",
        "    for j in neg:\n",
        "      if(i==j):\n",
        "        negg=+1\n",
        "        # print(j)\n",
        "\n",
        "  # Polarity Score = (Positive Score â€“ Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
        "  pscore=(poss-negg)/((poss+negg)+0.000001)\n",
        "  print('Polarity Score =',pscore)\n",
        "  # Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
        "  sscore=(poss+negg)/((len(tokens))+0.000001)\n",
        "  print('Subjectivity Score = ',sscore)\n",
        "  # Average Sentence Length = the number of words / the number of sentences\n",
        "  slen=len(tokens)/len(sent)\n",
        "  print('Average Sentence Length =',slen)\n",
        "  # Percentage of Complex words = the number of complex words / the number of words \n",
        "  for i in tokens:\n",
        "    x=syllable_count(i)\n",
        "    if (x>2):\n",
        "      cmplex=+1\n",
        "  comp_word_naalysis=cmplex/len(tokens)\n",
        "  print('Percentage of Complex words =',comp_word_naalysis)\n",
        "  # Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
        "  fog_index= 0.4 * (slen/comp_word_naalysis)\n",
        "  print('Fog Index =',fog_index)\n",
        "  # Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
        "  for xxe in se:\n",
        "    x=nltk.word_tokenize(xxe[0])\n",
        "    se1.append(len(x))\n",
        "  w_per_sen=np.average(se1)\n",
        "  print('Average Number of Words Per Sentence =',w_per_sen)\n",
        "\n",
        "  # df.append({'URL':url,\n",
        "  #       'Polarity Score':pscore,\n",
        "  #       'Subjectivity Score':sscore,\n",
        "  #       'Average Sentence Length':slen,\n",
        "  #       '% of Complex words':comp_word_naalysis,\n",
        "  #       'Fog Index':fog_index,\n",
        "  #       'Average Words/Sentence':w_per_sen},ignore_index=True)\n",
        "\n",
        "  return ({'URL':url,\n",
        "        'Polarity Score':pscore,\n",
        "        'Subjectivity Score':sscore,\n",
        "        'Average Sentence Length':slen,\n",
        "        '% of Complex words':comp_word_naalysis,\n",
        "        'Fog Index':fog_index,\n",
        "        'Average Words/Sentence':w_per_sen})"
      ],
      "metadata": {
        "id": "wKVeVyQM9AHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the data from the url\n",
        "def getdata(urll):\n",
        "  urlstr=''\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
        "  r = requests.get(urll)\n",
        "  htmlContent = r.content\n",
        "  soup = BeautifulSoup(requests.get(urll, headers=headers).content, \"html.parser\")\n",
        "  paras = soup.find_all('p')\n",
        "  for para in paras:\n",
        "      # print(str(para.text.encode(\"utf-8\")))\n",
        "      urlstr=urlstr+ str(para.text.encode(\"utf-8\"))\n",
        "  return(urlstr)"
      ],
      "metadata": {
        "id": "iqt15LIV9uaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(1,sheet.nrows):\n",
        "for i in range(1,3):\n",
        "  print(str(i)+\") \"+sheet.cell_value(i, 1))\n",
        "  urlst=getdata(sheet.cell_value(i, 1))\n",
        "  x=score(urlst,str(sheet.cell_value(i, 1)))\n",
        "  print(x)\n",
        "  df.append(x,ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4zjHa2S6zJF",
        "outputId": "54d53321-072b-4cf3-85d4-4c121bbbd9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/\n",
            "Polarity Score = 0.0\n",
            "Subjectivity Score =  0.0010576414589859114\n",
            "Average Sentence Length = 25.904109589041095\n",
            "Percentage of Complex words = 0.0005288207297726071\n",
            "Fog Index = 19593.868493150683\n",
            "Average Number of Words Per Sentence = 27.397260273972602\n",
            "2) https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/\n",
            "Polarity Score = 0.0\n",
            "Subjectivity Score =  0.0013351134837549308\n",
            "Average Sentence Length = 19.710526315789473\n",
            "Percentage of Complex words = 0.0006675567423230974\n",
            "Fog Index = 11810.547368421052\n",
            "Average Number of Words Per Sentence = 21.105263157894736\n"
          ]
        }
      ]
    }
  ]
}